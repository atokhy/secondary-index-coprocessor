<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=ISO-8859-1">
   <title>Chapter&nbsp;1.&nbsp;HBase Operational Management</title><link rel="stylesheet" type="text/css" href="css/freebsd_docbook.css"><meta name="generator" content="DocBook XSL-NS Stylesheets V1.76.1"></head><body bgcolor="white" text="black" link="#0000FF" vlink="#840084" alink="#0000FF"><div class="chapter" title="Chapter&nbsp;1.&nbsp;HBase Operational Management"><div class="titlepage"><div><div><h2 class="title"><a name="ops_mgt"></a>Chapter&nbsp;1.&nbsp;HBase Operational Management</h2></div></div></div><div class="toc"><p><b>Table of Contents</b></p><dl><dt><span class="section"><a href="#tools">1.1. HBase Tools and Utilities</a></span></dt><dd><dl><dt><span class="section"><a href="#hbck">1.1.1. HBase <span class="application">hbck</span></a></span></dt><dt><span class="section"><a href="#hfile_tool2">1.1.2. HFile Tool</a></span></dt><dt><span class="section"><a href="#wal_tools">1.1.3. WAL Tools</a></span></dt><dt><span class="section"><a href="#compression.tool">1.1.4. Compression Tool</a></span></dt><dt><span class="section"><a href="#copytable">1.1.5. CopyTable</a></span></dt><dt><span class="section"><a href="#export">1.1.6. Export</a></span></dt><dt><span class="section"><a href="#import">1.1.7. Import</a></span></dt><dt><span class="section"><a href="#walplayer">1.1.8. WALPlayer</a></span></dt><dt><span class="section"><a href="#rowcounter">1.1.9. RowCounter</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.regionmgt">1.2. Region Management</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.regionmgt.majorcompact">1.2.1. Major Compaction</a></span></dt><dt><span class="section"><a href="#ops.regionmgt.merge">1.2.2. Merge</a></span></dt></dl></dd><dt><span class="section"><a href="#node.management">1.3. Node Management</a></span></dt><dd><dl><dt><span class="section"><a href="#decommission">1.3.1. Node Decommission</a></span></dt><dt><span class="section"><a href="#rolling">1.3.2. Rolling Restart</a></span></dt></dl></dd><dt><span class="section"><a href="#hbase_metrics">1.4. Metrics</a></span></dt><dd><dl><dt><span class="section"><a href="#metric_setup">1.4.1. Metric Setup</a></span></dt><dt><span class="section"><a href="#rs_metrics">1.4.2. RegionServer Metrics</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.monitoring">1.5. HBase Monitoring</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.slow.query">1.5.1. Slow Query Log</a></span></dt></dl></dd><dt><span class="section"><a href="#cluster_replication">1.6. Cluster Replication</a></span></dt><dt><span class="section"><a href="#ops.backup">1.7. HBase Backup</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.backup.fullshutdown">1.7.1. Full Shutdown Backup</a></span></dt><dt><span class="section"><a href="#ops.backup.live.replication">1.7.2. Live Cluster Backup - Replication</a></span></dt><dt><span class="section"><a href="#ops.backup.live.copytable">1.7.3. Live Cluster Backup - CopyTable</a></span></dt><dt><span class="section"><a href="#ops.backup.live.export">1.7.4. Live Cluster Backup - Export</a></span></dt></dl></dd><dt><span class="section"><a href="#ops.capacity">1.8. Capacity Planning</a></span></dt><dd><dl><dt><span class="section"><a href="#ops.capacity.storage">1.8.1. Storage</a></span></dt><dt><span class="section"><a href="#ops.capacity.regions">1.8.2. Regions</a></span></dt></dl></dd></dl></div>
  This chapter will cover operational tools and practices required of a running HBase cluster.
  The subject of operations is related to the topics of <a class="xref" href="#">???</a>, <a class="xref" href="#">???</a>,
  and <a class="xref" href="#">???</a> but is a distinct topic in itself.  
  
  <div class="section" title="1.1.&nbsp;HBase Tools and Utilities"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="tools"></a>1.1.&nbsp;HBase Tools and Utilities</h2></div></div></div><p>Here we list HBase tools for administration, analysis, fixup, and
    debugging.</p><div class="section" title="1.1.1.&nbsp;HBase hbck"><div class="titlepage"><div><div><h3 class="title"><a name="hbck"></a>1.1.1.&nbsp;HBase <span class="application">hbck</span></h3></div><div><h4 class="subtitle">An <span class="emphasis"><em>fsck</em></span> for your HBase install</h4></div></div></div><p>To run <span class="application">hbck</span> against your HBase cluster run
        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
        At the end of the commands output it prints <span class="emphasis"><em>OK</em></span>
        or <span class="emphasis"><em>INCONSISTENCY</em></span>. If your cluster reports
        inconsistencies, pass <span class="command"><strong>-details</strong></span> to see more detail emitted.
        If inconsistencies, run <span class="command"><strong>hbck</strong></span> a few times because the
        inconsistency may be transient (e.g. cluster is starting up or a region is
        splitting).
        Passing <span class="command"><strong>-fix</strong></span> may correct the inconsistency (This latter
        is an experimental feature).
        </p></div><div class="section" title="1.1.2.&nbsp;HFile Tool"><div class="titlepage"><div><div><h3 class="title"><a name="hfile_tool2"></a>1.1.2.&nbsp;HFile Tool</h3></div></div></div><p>See <a class="xref" href="#">???</a>.</p></div><div class="section" title="1.1.3.&nbsp;WAL Tools"><div class="titlepage"><div><div><h3 class="title"><a name="wal_tools"></a>1.1.3.&nbsp;WAL Tools</h3></div></div></div><div class="section" title="1.1.3.1.&nbsp;HLog tool"><div class="titlepage"><div><div><h4 class="title"><a name="hlog_tool"></a>1.1.3.1.&nbsp;<code class="classname">HLog</code> tool</h4></div></div></div><p>The main method on <code class="classname">HLog</code> offers manual
        split and dump facilities. Pass it WALs or the product of a split, the
        content of the <code class="filename">recovered.edits</code>. directory.</p><p>You can get a textual dump of a WAL file content by doing the
        following:</p><pre class="programlisting"> <code class="code">$ ./bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --dump hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/10.10.21.10%3A60020.1283973724012</code> </pre><p>The
        return code will be non-zero if issues with the file so you can test
        wholesomeness of file by redirecting <code class="varname">STDOUT</code> to
        <code class="code">/dev/null</code> and testing the program return.</p><p>Similarly you can force a split of a log file directory by
        doing:</p><pre class="programlisting"> $ ./<code class="code">bin/hbase org.apache.hadoop.hbase.regionserver.wal.HLog --split hdfs://example.org:8020/hbase/.logs/example.org,60020,1283516293161/</code></pre></div></div><div class="section" title="1.1.4.&nbsp;Compression Tool"><div class="titlepage"><div><div><h3 class="title"><a name="compression.tool"></a>1.1.4.&nbsp;Compression Tool</h3></div></div></div><p>See <a class="xref" href="#compression.tool" title="1.1.4.&nbsp;Compression Tool">Section&nbsp;1.1.4, &#8220;Compression Tool&#8221;</a>.</p></div><div class="section" title="1.1.5.&nbsp;CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="copytable"></a>1.1.5.&nbsp;CopyTable</h3></div></div></div><p>
            CopyTable is a utility that can copy part or of all of a table, either to the same cluster or another cluster. The usage is as follows:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable [--starttime=X] [--endtime=Y] [--new.name=NEW] [--peer.adr=ADR] tablename
</pre><p>
        </p><p>
        Options:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">starttime</code>  Beginning of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">endtime</code>  End of the time range.  Without endtime means starttime to forever.</li><li class="listitem"><code class="varname">versions</code>  Number of cell versions to copy.</li><li class="listitem"><code class="varname">new.name</code>  New table's name.</li><li class="listitem"><code class="varname">peer.adr</code>  Address of the peer cluster given in the format hbase.zookeeper.quorum:hbase.zookeeper.client.port:zookeeper.znode.parent</li><li class="listitem"><code class="varname">families</code>  Comma-separated list of ColumnFamilies to copy.</li><li class="listitem"><code class="varname">all.cells</code>  Also copy delete markers and uncollected deleted cells (advanced option).</li></ul></div><p>
         Args:
        </p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem">tablename  Name of table to copy.</li></ul></div><p>
        </p><p>Example of copying 'TestTable' to a cluster that uses replication for a 1 hour window:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable</pre><p>
        </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="1.1.6.&nbsp;Export"><div class="titlepage"><div><div><h3 class="title"><a name="export"></a>1.1.6.&nbsp;Export</h3></div></div></div><p>Export is a utility that will dump the contents of table to HDFS in a sequence file.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
        </p></div><div class="section" title="1.1.7.&nbsp;Import"><div class="titlepage"><div><div><h3 class="title"><a name="import"></a>1.1.7.&nbsp;Import</h3></div></div></div><p>Import is a utility that will load data that has been exported back into HBase.  Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;
</pre><p>
       </p></div><div class="section" title="1.1.8.&nbsp;WALPlayer"><div class="titlepage"><div><div><h3 class="title"><a name="walplayer"></a>1.1.8.&nbsp;WALPlayer</h3></div></div></div><p>WALPlayer is a utility to replay WAL files into HBase.
       </p><p>The WAL can be replayed for a set of tables or all tables, and a timerange can be provided (in milliseconds). The WAL is filtered to this set of tables. The output can optionally be mapped to another set of tables.
       </p><p>WALPlayer can also generate HFiles for later bulk importing, in that case only a single table and no mapping can be specified.
       </p><p>Invoke via:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer [options] &lt;wal inputdir&gt; &lt;tables&gt; [&lt;tableMappings&gt;]&gt;
</pre><p>
       </p><p>For example:
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.WALPlayer /backuplogdir oldTable1,oldTable2 newTable1,newTable2
</pre><p>
       </p></div><div class="section" title="1.1.9.&nbsp;RowCounter"><div class="titlepage"><div><div><h3 class="title"><a name="rowcounter"></a>1.1.9.&nbsp;RowCounter</h3></div></div></div><p>RowCounter is a utility that will count all the rows of a table.  This is a good utility to use
       as a sanity check to ensure that HBase can read all the blocks of a table if there are any concerns of metadata inconsistency.
</p><pre class="programlisting">$ bin/hbase org.apache.hadoop.hbase.mapreduce.RowCounter &lt;tablename&gt; [&lt;column1&gt; &lt;column2&gt;...]
</pre><p>
       </p><p>Note:  caching for the input Scan is configured via <code class="code">hbase.client.scanner.caching</code> in the job configuration.
       </p></div></div><div class="section" title="1.2.&nbsp;Region Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.regionmgt"></a>1.2.&nbsp;Region Management</h2></div></div></div><div class="section" title="1.2.1.&nbsp;Major Compaction"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.majorcompact"></a>1.2.1.&nbsp;Major Compaction</h3></div></div></div><p>Major compactions can be requested via the HBase shell or <a class="link" href="http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/client/HBaseAdmin.html#majorCompact%28java.lang.String%29" target="_top">HBaseAdmin.majorCompact</a>.
      </p><p>Note:  major compactions do NOT do region merges.  See <a class="xref" href="#">???</a> for more information about compactions.
      
      </p></div><div class="section" title="1.2.2.&nbsp;Merge"><div class="titlepage"><div><div><h3 class="title"><a name="ops.regionmgt.merge"></a>1.2.2.&nbsp;Merge</h3></div></div></div><p>Merge is a utility that can merge adjoining regions in the same table (see org.apache.hadoop.hbase.util.Merge).</p><pre class="programlisting">$ bin/hbase org.apache.hbase.util.Merge &lt;tablename&gt; &lt;region1&gt; &lt;region2&gt;
</pre><p>If you feel you have too many regions and want to consolidate them, Merge is the utility you need.  Merge must
      run be done when the cluster is down.  
      See the <a class="link" href="http://ofps.oreilly.com/titles/9781449396107/performance.html" target="_top">O'Reilly HBase Book</a> for
      an example of usage.
      </p><p>Additionally, there is a Ruby script attached to <a class="link" href="https://issues.apache.org/jira/browse/HBASE-1621" target="_top">HBASE-1621</a> 
      for region merging.
      </p></div></div><div class="section" title="1.3.&nbsp;Node Management"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="node.management"></a>1.3.&nbsp;Node Management</h2></div></div></div><div class="section" title="1.3.1.&nbsp;Node Decommission"><div class="titlepage"><div><div><h3 class="title"><a name="decommission"></a>1.3.1.&nbsp;Node Decommission</h3></div></div></div><p>You can stop an individual RegionServer by running the following
            script in the HBase directory on the particular  node:
            </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop regionserver</pre><p>
            The RegionServer will first close all regions and then shut itself down.
            On shutdown, the RegionServer's ephemeral node in ZooKeeper will expire.
            The master will notice the RegionServer gone and will treat it as
            a 'crashed' server; it will reassign the nodes the RegionServer was carrying.
            </p><div class="note" title="Disable the Load Balancer before Decommissioning a node" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">Disable the Load Balancer before Decommissioning a node</h3><p>If the load balancer runs while a node is shutting down, then
                 there could be contention between the Load Balancer and the
                 Master's recovery of the just decommissioned RegionServer.
                 Avoid any problems by disabling the balancer first.
                 See <a class="xref" href="#lb" title="Load Balancer">Load Balancer</a> below.
             </p></div><p>
        </p><p>
        A downside to the above stop of a RegionServer is that regions could be offline for
        a good period of time.  Regions are closed in order.  If many regions on the server, the
        first region to close may not be back online until all regions close and after the master
        notices the RegionServer's znode gone.  In HBase 0.90.2, we added facility for having
        a node gradually shed its load and then shutdown itself down.  HBase 0.90.2 added the
            <code class="filename">graceful_stop.sh</code> script.  Here is its usage:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh 
Usage: graceful_stop.sh [--config &amp;conf-dir&gt;] [--restart] [--reload] [--thrift] [--rest] &amp;hostname&gt;
 thrift      If we should stop/start thrift before/after the hbase stop/start
 rest        If we should stop/start rest before/after the hbase stop/start
 restart     If we should restart after graceful stop
 reload      Move offloaded regions back on to the stopped server
 debug       Move offloaded regions back on to the stopped server
 hostname    Hostname of server we are to stop</pre><p>
        </p><p>
            To decommission a loaded RegionServer, run the following:
            </p><pre class="programlisting">$ ./bin/graceful_stop.sh HOSTNAME</pre><p>
            where <code class="varname">HOSTNAME</code> is the host carrying the RegionServer
            you would decommission.  
            </p><div class="note" title="On HOSTNAME" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title">On <code class="varname">HOSTNAME</code></h3><p>The <code class="varname">HOSTNAME</code> passed to <code class="filename">graceful_stop.sh</code>
            must match the hostname that hbase is using to identify RegionServers.
            Check the list of RegionServers in the master UI for how HBase is
            referring to servers. Its usually hostname but can also be FQDN.
            Whatever HBase is using, this is what you should pass the
            <code class="filename">graceful_stop.sh</code> decommission
            script.  If you pass IPs, the script is not yet smart enough to make
            a hostname (or FQDN) of it and so it will fail when it checks if server is
            currently running; the graceful unloading of regions will not run.
            </p></div><p> The <code class="filename">graceful_stop.sh</code> script will move the regions off the
            decommissioned RegionServer one at a time to minimize region churn.
            It will verify the region deployed in the new location before it
            will moves the next region and so on until the decommissioned server
            is carrying zero regions.  At this point, the <code class="filename">graceful_stop.sh</code>
            tells the RegionServer <span class="command"><strong>stop</strong></span>.  The master will at this point notice the
            RegionServer gone but all regions will have already been redeployed
            and because the RegionServer went down cleanly, there will be no
            WAL logs to split.
            </p><div class="note" title="Load Balancer" style="margin-left: 0.5in; margin-right: 0.5in;"><h3 class="title"><a name="lb"></a>Load Balancer</h3><p> 
                It is assumed that the Region Load Balancer is disabled while the
                <span class="command"><strong>graceful_stop</strong></span> script runs (otherwise the balancer
                and the decommission script will end up fighting over region deployments).
                Use the shell to disable the balancer:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch false
true
0 row(s) in 0.3590 seconds</pre><p>
This turns the balancer OFF.  To reenable, do:
                </p><pre class="programlisting">hbase(main):001:0&gt; balance_switch true
false
0 row(s) in 0.3590 seconds</pre><p>
            </p></div><p>
        </p></div><div class="section" title="1.3.2.&nbsp;Rolling Restart"><div class="titlepage"><div><div><h3 class="title"><a name="rolling"></a>1.3.2.&nbsp;Rolling Restart</h3></div></div></div><p>
            You can also ask this script to restart a RegionServer after the shutdown
            AND move its old regions back into place.  The latter you might do to
            retain data locality.  A primitive rolling restart might be effected by
            running something like the following:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
            Tail the output of <code class="filename">/tmp/log.txt</code> to follow the scripts
            progress. The above does RegionServers only.  Be sure to disable the
            load balancer before doing the above.  You'd need to do the master
            update separately.  Do it before you run the above script.
            Here is a pseudo-script for how you might craft a rolling restart script:
            </p><div class="orderedlist"><ol class="orderedlist" type="1"><li class="listitem"><p>Untar your release, make sure of its configuration and
                        then rsync it across the cluster. If this is 0.90.2, patch it
                        with HBASE-3744 and HBASE-3756.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster consistent
                        </p><pre class="programlisting">$ ./bin/hbase hbck</pre><p>
                    Effect repairs if inconsistent.
                    </p></li><li class="listitem"><p>Restart the Master: </p><pre class="programlisting">$ ./bin/hbase-daemon.sh stop master; ./bin/hbase-daemon.sh start master</pre><p>
                    </p></li><li class="listitem"><p>
                       Disable the region balancer:</p><pre class="programlisting">$ echo "balance_switch false" | ./bin/hbase shell</pre><p>
                    </p></li><li class="listitem"><p>Run the <code class="filename">graceful_stop.sh</code> script per RegionServer.  For example:
            </p><pre class="programlisting">$ for i in `cat conf/regionservers|sort`; do ./bin/graceful_stop.sh --restart --reload --debug $i; done &amp;&gt; /tmp/log.txt &amp;
            </pre><p>
                     If you are running thrift or rest servers on the RegionServer, pass --thrift or --rest options (See usage
                     for <code class="filename">graceful_stop.sh</code> script).
                 </p></li><li class="listitem"><p>Restart the Master again.  This will clear out dead servers list and reenable the balancer.
                    </p></li><li class="listitem"><p>Run hbck to ensure the cluster is consistent.
                    </p></li></ol></div><p>
        </p></div></div><div class="section" title="1.4.&nbsp;Metrics"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="hbase_metrics"></a>1.4.&nbsp;Metrics</h2></div></div></div><div class="section" title="1.4.1.&nbsp;Metric Setup"><div class="titlepage"><div><div><h3 class="title"><a name="metric_setup"></a>1.4.1.&nbsp;Metric Setup</h3></div></div></div><p>See <a class="link" href="http://hbase.apache.org/metrics.html" target="_top">Metrics</a> for
  an introduction and how to enable Metrics emission.
  </p></div><div class="section" title="1.4.2.&nbsp;RegionServer Metrics"><div class="titlepage"><div><div><h3 class="title"><a name="rs_metrics"></a>1.4.2.&nbsp;RegionServer Metrics</h3></div></div></div><div class="section" title="1.4.2.1.&nbsp;hbase.regionserver.blockCacheCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheCount"></a>1.4.2.1.&nbsp;<code class="varname">hbase.regionserver.blockCacheCount</code></h4></div></div></div><p>Block cache item count in memory.  This is the number of blocks of StoreFiles (HFiles) in the cache.</p></div><div class="section" title="1.4.2.2.&nbsp;hbase.regionserver.blockCacheEvictedCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheEvictedCount"></a>1.4.2.2.&nbsp;<code class="varname">hbase.regionserver.blockCacheEvictedCount</code></h4></div></div></div><p>Number of blocks that had to be evicted from the block cache due to heap size constraints.</p></div><div class="section" title="1.4.2.3.&nbsp;hbase.regionserver.blockCacheFree"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheFree"></a>1.4.2.3.&nbsp;<code class="varname">hbase.regionserver.blockCacheFree</code></h4></div></div></div><p>Block cache memory available (bytes).</p></div><div class="section" title="1.4.2.4.&nbsp;hbase.regionserver.blockCacheHitCachingRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCachingRatio"></a>1.4.2.4.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitCachingRatio</code></h4></div></div></div><p>Block cache hit caching ratio (0 to 100).  The cache-hit ratio for reads configured to look in the cache (i.e., cacheBlocks=true). </p></div><div class="section" title="1.4.2.5.&nbsp;hbase.regionserver.blockCacheHitCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitCount"></a>1.4.2.5.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) read from the cache.</p></div><div class="section" title="1.4.2.6.&nbsp;hbase.regionserver.blockCacheHitRatio"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheHitRatio"></a>1.4.2.6.&nbsp;<code class="varname">hbase.regionserver.blockCacheHitRatio</code></h4></div></div></div><p>Block cache hit ratio (0 to 100).  Includes all read requests, although those with cacheBlocks=false
           will always read from disk and be counted as a "cache miss".</p></div><div class="section" title="1.4.2.7.&nbsp;hbase.regionserver.blockCacheMissCount"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheMissCount"></a>1.4.2.7.&nbsp;<code class="varname">hbase.regionserver.blockCacheMissCount</code></h4></div></div></div><p>Number of blocks of StoreFiles (HFiles) requested but not read from the cache.</p></div><div class="section" title="1.4.2.8.&nbsp;hbase.regionserver.blockCacheSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.blockCacheSize"></a>1.4.2.8.&nbsp;<code class="varname">hbase.regionserver.blockCacheSize</code></h4></div></div></div><p>Block cache size in memory (bytes).  i.e., memory in use by the BlockCache</p></div><div class="section" title="1.4.2.9.&nbsp;hbase.regionserver.compactionQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.compactionQueueSize"></a>1.4.2.9.&nbsp;<code class="varname">hbase.regionserver.compactionQueueSize</code></h4></div></div></div><p>Size of the compaction queue.  This is the number of Stores in the RegionServer that have been targeted for compaction.</p></div><div class="section" title="1.4.2.10.&nbsp;hbase.regionserver.flushQueueSize"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.flushQueueSize"></a>1.4.2.10.&nbsp;<code class="varname">hbase.regionserver.flushQueueSize</code></h4></div></div></div><p>Number of enqueued regions in the MemStore awaiting flush.</p></div><div class="section" title="1.4.2.11.&nbsp;hbase.regionserver.fsReadLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency_avg_time"></a>1.4.2.11.&nbsp;<code class="varname">hbase.regionserver.fsReadLatency_avg_time</code></h4></div></div></div><p>Filesystem read latency (ms).  This is the average time to read from HDFS.</p></div><div class="section" title="1.4.2.12.&nbsp;hbase.regionserver.fsReadLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsReadLatency_num_ops"></a>1.4.2.12.&nbsp;<code class="varname">hbase.regionserver.fsReadLatency_num_ops</code></h4></div></div></div><p>Filesystem read operations.</p></div><div class="section" title="1.4.2.13.&nbsp;hbase.regionserver.fsSyncLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsSyncLatency_avg_time"></a>1.4.2.13.&nbsp;<code class="varname">hbase.regionserver.fsSyncLatency_avg_time</code></h4></div></div></div><p>Filesystem sync latency (ms).  Latency to sync the write-ahead log records to the filesystem.</p></div><div class="section" title="1.4.2.14.&nbsp;hbase.regionserver.fsSyncLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsSyncLatency_num_ops"></a>1.4.2.14.&nbsp;<code class="varname">hbase.regionserver.fsSyncLatency_num_ops</code></h4></div></div></div><p>Number of operations to sync the write-ahead log records to the filesystem.</p></div><div class="section" title="1.4.2.15.&nbsp;hbase.regionserver.fsWriteLatency_avg_time"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency_avg_time"></a>1.4.2.15.&nbsp;<code class="varname">hbase.regionserver.fsWriteLatency_avg_time</code></h4></div></div></div><p>Filesystem write latency (ms).  Total latency for all writers, including StoreFiles and write-head log.</p></div><div class="section" title="1.4.2.16.&nbsp;hbase.regionserver.fsWriteLatency_num_ops"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.fsWriteLatency_num_ops"></a>1.4.2.16.&nbsp;<code class="varname">hbase.regionserver.fsWriteLatency_num_ops</code></h4></div></div></div><p>Number of filesystem write operations, including StoreFiles and write-ahead log.</p></div><div class="section" title="1.4.2.17.&nbsp;hbase.regionserver.memstoreSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.memstoreSizeMB"></a>1.4.2.17.&nbsp;<code class="varname">hbase.regionserver.memstoreSizeMB</code></h4></div></div></div><p>Sum of all the memstore sizes in this RegionServer (MB)</p></div><div class="section" title="1.4.2.18.&nbsp;hbase.regionserver.regions"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.regions"></a>1.4.2.18.&nbsp;<code class="varname">hbase.regionserver.regions</code></h4></div></div></div><p>Number of regions served by the RegionServer</p></div><div class="section" title="1.4.2.19.&nbsp;hbase.regionserver.requests"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.requests"></a>1.4.2.19.&nbsp;<code class="varname">hbase.regionserver.requests</code></h4></div></div></div><p>Total number of read and write requests.  Requests correspond to RegionServer RPC calls, thus a single Get will result in 1 request, but a Scan with caching set to 1000 will result in 1 request for each 'next' call (i.e., not each row).  A bulk-load request will constitute 1 request per HFile.</p></div><div class="section" title="1.4.2.20.&nbsp;hbase.regionserver.storeFileIndexSizeMB"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFileIndexSizeMB"></a>1.4.2.20.&nbsp;<code class="varname">hbase.regionserver.storeFileIndexSizeMB</code></h4></div></div></div><p>Sum of all the StoreFile index sizes in this RegionServer (MB)</p></div><div class="section" title="1.4.2.21.&nbsp;hbase.regionserver.stores"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.stores"></a>1.4.2.21.&nbsp;<code class="varname">hbase.regionserver.stores</code></h4></div></div></div><p>Number of Stores open on the RegionServer.  A Store corresponds to a ColumnFamily.  For example, if a table (which contains the column family) has 3 regions on a RegionServer, there will be 3 stores open for that column family. </p></div><div class="section" title="1.4.2.22.&nbsp;hbase.regionserver.storeFiles"><div class="titlepage"><div><div><h4 class="title"><a name="hbase.regionserver.storeFiles"></a>1.4.2.22.&nbsp;<code class="varname">hbase.regionserver.storeFiles</code></h4></div></div></div><p>Number of StoreFiles open on the RegionServer.  A store may have more than one StoreFile (HFile).</p></div></div></div><div class="section" title="1.5.&nbsp;HBase Monitoring"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.monitoring"></a>1.5.&nbsp;HBase Monitoring</h2></div></div></div><p>TODO
    </p><div class="section" title="1.5.1.&nbsp;Slow Query Log"><div class="titlepage"><div><div><h3 class="title"><a name="ops.slow.query"></a>1.5.1.&nbsp;Slow Query Log</h3></div></div></div><p>The HBase slow query log consists of parseable JSON structures describing the properties of those client operations (Gets, Puts, Deletes, etc.) that either took too long to run, or produced too much output. The thresholds for "too long to run" and "too much output" are configurable, as described below. The output is produced inline in the main region server logs so that it is easy to discover further details from context with other logged events. It is also prepended with identifying tags <code class="constant">(responseTooSlow)</code>, <code class="constant">(responseTooLarge)</code>, <code class="constant">(operationTooSlow)</code>, and <code class="constant">(operationTooLarge)</code> in order to enable easy filtering with grep, in case the user desires to see only slow queries.
</p><div class="section" title="1.5.1.1.&nbsp;Configuration"><div class="titlepage"><div><div><h4 class="title"><a name="d0e537"></a>1.5.1.1.&nbsp;Configuration</h4></div></div></div><p>There are two configuration knobs that can be used to adjust the thresholds for when queries are logged.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hbase.ipc.warn.response.time</code> Maximum number of milliseconds that a query can be run without being logged. Defaults to 10000, or 10 seconds. Can be set to -1 to disable logging by time.
</li><li class="listitem"><code class="varname">hbase.ipc.warn.response.size</code> Maximum byte size of response that a query can return without being logged. Defaults to 100 megabytes. Can be set to -1 to disable logging by size.
</li></ul></div></div><div class="section" title="1.5.1.2.&nbsp;Metrics"><div class="titlepage"><div><div><h4 class="title"><a name="d0e551"></a>1.5.1.2.&nbsp;Metrics</h4></div></div></div><p>The slow query log exposes to metrics to JMX.
</p><div class="itemizedlist"><ul class="itemizedlist" type="disc"><li class="listitem"><code class="varname">hadoop.regionserver_rpc_slowResponse</code> a global metric reflecting the durations of all responses that triggered logging.</li><li class="listitem"><code class="varname">hadoop.regionserver_rpc_methodName.aboveOneSec</code> A metric reflecting the durations of all responses that lasted for more than one second.</li></ul></div><p>
</p></div><div class="section" title="1.5.1.3.&nbsp;Output"><div class="titlepage"><div><div><h4 class="title"><a name="d0e566"></a>1.5.1.3.&nbsp;Output</h4></div></div></div><p>The output is tagged with operation e.g. <code class="constant">(operationTooSlow)</code> if the call was a client operation, such as a Put, Get, or Delete, which we expose detailed fingerprint information for. If not, it is tagged <code class="constant">(responseTooSlow)</code> and still produces parseable JSON output, but with less verbose information solely regarding its duration and size in the RPC itself. <code class="constant">TooLarge</code> is substituted for <code class="constant">TooSlow</code> if the response size triggered the logging, with <code class="constant">TooLarge</code> appearing even in the case that both size and duration triggered logging.
</p></div><div class="section" title="1.5.1.4.&nbsp;Example"><div class="titlepage"><div><div><h4 class="title"><a name="d0e586"></a>1.5.1.4.&nbsp;Example</h4></div></div></div><p>
</p><pre class="programlisting">2011-09-08 10:01:25,824 WARN org.apache.hadoop.ipc.HBaseServer: (operationTooSlow): {"tables":{"riley2":{"puts":[{"totalColumns":11,"families":{"actions":[{"timestamp":1315501284459,"qualifier":"0","vlen":9667580},{"timestamp":1315501284459,"qualifier":"1","vlen":10122412},{"timestamp":1315501284459,"qualifier":"2","vlen":11104617},{"timestamp":1315501284459,"qualifier":"3","vlen":13430635}]},"row":"cfcd208495d565ef66e7dff9f98764da:0"}],"families":["actions"]}},"processingtimems":956,"client":"10.47.34.63:33623","starttimems":1315501284456,"queuetimems":0,"totalPuts":1,"class":"HRegionServer","responsesize":0,"method":"multiPut"}</pre><p>
</p><p>Note that everything inside the "tables" structure is output produced by MultiPut's fingerprint, while the rest of the information is RPC-specific, such as processing time and client IP/port. Other client operations follow the same pattern and the same general structure, with necessary differences due to the nature of the individual operations. In the case that the call is not a client operation, that detailed fingerprint information will be completely absent.
</p><p>This particular example, for example, would indicate that the likely cause of slowness is simply a very large (on the order of 100MB) multiput, as we can tell by the "vlen," or value length, fields of each put in the multiPut.
</p></div></div></div><div class="section" title="1.6.&nbsp;Cluster Replication"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="cluster_replication"></a>1.6.&nbsp;Cluster Replication</h2></div></div></div><p>See <a class="link" href="http://hbase.apache.org/replication.html" target="_top">Cluster Replication</a>.
    </p></div><div class="section" title="1.7.&nbsp;HBase Backup"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.backup"></a>1.7.&nbsp;HBase Backup</h2></div></div></div><p>There are two broad strategies for performing HBase backups: backing up with a full cluster shutdown, and backing up on a live cluster. 
    Each approach has pros and cons.   
    </p><p>For additional information, see <a class="link" href="http://blog.sematext.com/2011/03/11/hbase-backup-options/" target="_top">HBase Backup Options</a> over on the Sematext Blog.
    </p><div class="section" title="1.7.1.&nbsp;Full Shutdown Backup"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.fullshutdown"></a>1.7.1.&nbsp;Full Shutdown Backup</h3></div></div></div><p>Some environments can tolerate a periodic full shutdown of their HBase cluster, for example if it is being used a back-end analytic capacity
      and not serving front-end web-pages.  The benefits are that the NameNode/Master are RegionServers are down, so there is no chance of missing
      any in-flight changes to either StoreFiles or metadata.  The obvious con is that the cluster is down.  The steps include:
      </p><div class="section" title="1.7.1.1.&nbsp;Stop HBase"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.stop"></a>1.7.1.1.&nbsp;Stop HBase</h4></div></div></div><p>
        </p></div><div class="section" title="1.7.1.2.&nbsp;Distcp"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.distcp"></a>1.7.1.2.&nbsp;Distcp</h4></div></div></div><p>Distcp could be used to either copy the contents of the HBase directory in HDFS to either the same cluster in another directory, or 
        to a different cluster.
        </p><p>Note:  Distcp works in this situation because the cluster is down and there are no in-flight edits to files.  
        Distcp-ing of files in the HBase directory is not generally recommended on a live cluster.
        </p></div><div class="section" title="1.7.1.3.&nbsp;Restore (if needed)"><div class="titlepage"><div><div><h4 class="title"><a name="ops.backup.fullshutdown.restore"></a>1.7.1.3.&nbsp;Restore (if needed)</h4></div></div></div><p>The backup of the hbase directory from HDFS is copied onto the 'real' hbase directory via distcp.  The act of copying these files 
        creates new HDFS metadata, which is why a restore of the NameNode edits from the time of the HBase backup isn't required for this kind of
        restore, because it's a restore (via distcp) of a specific HDFS directory (i.e., the HBase part) not the entire HDFS file-system.
        </p></div></div><div class="section" title="1.7.2.&nbsp;Live Cluster Backup - Replication"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.replication"></a>1.7.2.&nbsp;Live Cluster Backup - Replication</h3></div></div></div><p>This approach assumes that there is a second cluster.  
      See the HBase page on <a class="link" href="http://hbase.apache.org/replication.html" target="_top">replication</a> for more information.
      </p></div><div class="section" title="1.7.3.&nbsp;Live Cluster Backup - CopyTable"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.copytable"></a>1.7.3.&nbsp;Live Cluster Backup - CopyTable</h3></div></div></div><p>The <a class="xref" href="#copytable" title="1.1.5.&nbsp;CopyTable">Section&nbsp;1.1.5, &#8220;CopyTable&#8221;</a> utility could either be used to copy data from one table to another on the 
      same cluster, or to copy data to another table on another cluster.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the copy process.
      </p></div><div class="section" title="1.7.4.&nbsp;Live Cluster Backup - Export"><div class="titlepage"><div><div><h3 class="title"><a name="ops.backup.live.export"></a>1.7.4.&nbsp;Live Cluster Backup - Export</h3></div></div></div><p>The <a class="xref" href="#export" title="1.1.6.&nbsp;Export">Section&nbsp;1.1.6, &#8220;Export&#8221;</a> approach dumps the content of a table to HDFS on the same cluster.  To restore the data, the
      <a class="xref" href="#import" title="1.1.7.&nbsp;Import">Section&nbsp;1.1.7, &#8220;Import&#8221;</a> utility would be used.
      </p><p>Since the cluster is up, there is a risk that edits could be missed in the export process.
      </p></div></div><div class="section" title="1.8.&nbsp;Capacity Planning"><div class="titlepage"><div><div><h2 class="title" style="clear: both"><a name="ops.capacity"></a>1.8.&nbsp;Capacity Planning</h2></div></div></div><div class="section" title="1.8.1.&nbsp;Storage"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.storage"></a>1.8.1.&nbsp;Storage</h3></div></div></div><p>A common question for HBase administrators is estimating how much storage will be required for an HBase cluster.
      There are several apsects to consider, the most important of which is what data load into the cluster.  Start
      with a solid understanding of how HBase handles data internally (KeyValue).
      </p><div class="section" title="1.8.1.1.&nbsp;KeyValue"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.kv"></a>1.8.1.1.&nbsp;KeyValue</h4></div></div></div><p>HBase storage will be dominated by KeyValues.  See <a class="xref" href="#">???</a> and <a class="xref" href="#">???</a> for 
        how HBase stores data internally.  
        </p><p>It is critical to understand that there is a KeyValue instance for every attribute stored in a row, and the 
        rowkey-length, ColumnFamily name-length and attribute lengths will drive the size of the database more than any other
        factor.
        </p></div><div class="section" title="1.8.1.2.&nbsp;StoreFiles and Blocks"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.sf"></a>1.8.1.2.&nbsp;StoreFiles and Blocks</h4></div></div></div><p>KeyValue instances are aggregated into blocks, and the blocksize is configurable on a per-ColumnFamily basis.
        Blocks are aggregated into StoreFile's.  See <a class="xref" href="#">???</a>.
        </p></div><div class="section" title="1.8.1.3.&nbsp;HDFS Block Replication"><div class="titlepage"><div><div><h4 class="title"><a name="ops.capacity.storage.hdfs"></a>1.8.1.3.&nbsp;HDFS Block Replication</h4></div></div></div><p>Because HBase runs on top of HDFS, factor in HDFS block replication into storage calculations.
        </p></div></div><div class="section" title="1.8.2.&nbsp;Regions"><div class="titlepage"><div><div><h3 class="title"><a name="ops.capacity.regions"></a>1.8.2.&nbsp;Regions</h3></div></div></div><p>Another common question for HBase administrators is determining the right number of regions per
      RegionServer.  This affects both storage and hardware planning. See <a class="xref" href="#">???</a>.
      </p></div></div></div></body></html>